{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbenayoun/Generative-AI-Module-Dauphine/blob/main/copie_de_adversarial3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YroMaxFATRHG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from random import random\n",
        "\n",
        "cuda = torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiY3BiAiTRHK"
      },
      "source": [
        "## From adversarial examples to training robust models\n",
        "\n",
        "In the previous notebooks, we focused on methods for solving the maximization problem over perturbations; that is, to finding the solution to the problem\n",
        "\\begin{equation}\n",
        "\\DeclareMathOperator*{\\maximize}{maximize}\n",
        "\\maximize_{\\|\\delta\\| \\leq \\epsilon} \\ell(h_\\theta(x + \\delta), y).\n",
        "\\end{equation}\n",
        "\n",
        "In this notebook, we will focus on training a robust classifier. More precisly, we aim at solving following minimization problem, namely Adversarial Training:\n",
        "\\begin{equation}\n",
        "\\DeclareMathOperator*{\\minimize}{minimize}\n",
        "\\minimize_\\theta \\frac{1}{|S|} \\sum_{x,y \\in S} \\max_{\\|\\delta\\| \\leq \\epsilon} \\ell(h_\\theta(x + \\delta), y).\n",
        "\\end{equation}\n",
        "The order of the min-max operations is important here.  Specially, the max is inside the minimization, meaning that the adversary (trying to maximize the loss) gets to \"move\" _second_.  We assume, essentially, that the adversary has full knowledge of the classifier parameters $\\theta$, and that they get to specialize their attack to whatever parameters we have chosen in the outer minimization. The goal of the robust optimization formulation, therefore, is to ensure that the model cannot be attacked _even if_ the adversary has full knowledge of the model.  Of course, in practice we may want to make assumptions about the power of the adversary but it can be difficult to pin down a precise definition of what we mean by the \"power\" of the adversary, so extra care should be taken in evaluating models against possible \"realistic\" adversaries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QMXb08jTRHL"
      },
      "source": [
        "## Exercice 1\n",
        "1. Train a robust classifier using Adversarial Training with a specific norm\n",
        "2. Evaluate your classifier on natural and adversarial examples crafted with the norm of the training and other norms\n",
        "3. Make an analysis and conclude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w21alYjaTRHL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "670a71e7-97d6-49fb-aca1-bf766338b2d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./docs/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 44278943.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./docs/cifar-10-python.tar.gz to ./docs\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# load CIFAR10 dataset\n",
        "def load_cifar(split, batch_size):\n",
        "  train = True if split == 'train' else False\n",
        "  dataset = datasets.CIFAR10(\"./docs\", train=split, download=True, transform=transforms.ToTensor())\n",
        "  return DataLoader(dataset, batch_size=batch_size, shuffle=train)\n",
        "\n",
        "batch_size = 100\n",
        "train_loader = load_cifar('train', batch_size)\n",
        "test_loader = load_cifar('test', batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5awbg2Qst1h"
      },
      "outputs": [],
      "source": [
        "class ConvModel(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(ConvModel, self).__init__()\n",
        "    # First Convolutional Layer: Input channels = 3, Output channels = 32\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "    # First Max Pooling Layer: Reduces each dimension by a factor of 2\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "    # Second Convolutional Layer: Input channels = 32, Output channels = 64\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "    # Second Max Pooling Layer: Reduces each dimension by a factor of 2\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "    self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, 10)  # Assuming 10 output classes\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # First convolutional block: Conv -> ReLU -> MaxPool\n",
        "    x = self.pool1(nn.functional.relu(self.conv1(x)))\n",
        "    # Second convolutional block: Conv -> ReLU -> MaxPool\n",
        "    x = self.pool2(nn.functional.relu(self.conv2(x)))\n",
        "\n",
        "    # Flatten the output for the fully connected layers\n",
        "    x = x.view(-1, 64 * 8 * 8)  # Flattening the output from conv layers\n",
        "\n",
        "    # First fully connected block: FC -> ReLU\n",
        "    x = nn.functional.relu(self.fc1(x))\n",
        "    # Second fully connected block: FC -> ReLU\n",
        "    x = nn.functional.relu(self.fc2(x))\n",
        "    # Output layer: FC\n",
        "    x = self.fc3(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9B-ea_dTRHO"
      },
      "outputs": [],
      "source": [
        "class FastGradientSignMethod:\n",
        "\n",
        "  def __init__(self, model, eps):\n",
        "    self.model = model\n",
        "    self.eps = eps\n",
        "\n",
        "\n",
        "  def compute(self, x, y):\n",
        "    \"\"\" Construct FGSM adversarial perturbation for examples x\"\"\"\n",
        "    delta = torch.zeros_like(x, requires_grad=True)\n",
        "    # code here ...\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    output = self.model(x + delta)\n",
        "    loss = criterion(output, y)\n",
        "    loss.backward()\n",
        "    # Use variable.grad.detach() to retreive the gradient with respect to a loss\n",
        "    grad = delta.grad.detach()\n",
        "    delta = self.eps * grad.sign()\n",
        "\n",
        "    return delta\n",
        "\n",
        "class ProjectedGradientDescent:\n",
        "\n",
        "  def __init__(self, model, eps, alpha, num_iter):\n",
        "    self.model = model\n",
        "    self.eps = eps\n",
        "    self.alpha = alpha\n",
        "    self.num_iter = num_iter\n",
        "\n",
        "  def compute(self, x, y):\n",
        "    \"\"\" Construct PGD adversarial pertubration on the examples x.\"\"\"\n",
        "    delta = torch.zeros_like(x, requires_grad=True)\n",
        "    for _ in range(self.num_iter):\n",
        "      if delta.grad is not None:\n",
        "        delta.grad.zero_()\n",
        "      output = self.model(x + delta)\n",
        "      loss = nn.CrossEntropyLoss()(output, y)\n",
        "      loss.backward()\n",
        "      delta.data = delta.data + self.alpha * delta.grad.sign()\n",
        "      delta.data = torch.clamp(delta.data, -self.eps, self.eps)\n",
        "      delta.data = torch.clamp(x + delta.data, 0, 1) - x\n",
        "    delta = delta.detach()\n",
        "\n",
        "    return delta\n",
        "\n",
        "class ProjectedGradientDescentl2:\n",
        "\n",
        "  def __init__(self, model, eps, alpha, num_iter):\n",
        "    # code here ...\n",
        "    self.model = model\n",
        "    self.eps = eps\n",
        "    self.alpha = alpha\n",
        "    self.num_iter = num_iter\n",
        "\n",
        "  def compute(self, x, y):\n",
        "    \"\"\" Construct PGD adversarial pertubration on the examples x.\"\"\"\n",
        "    # code here ...\n",
        "    delta = torch.zeros_like(x, requires_grad=True)\n",
        "    for _ in range(self.num_iter):\n",
        "      output = self.model(x + delta)\n",
        "      loss = nn.CrossEntropyLoss()(output, y)\n",
        "      loss.backward()\n",
        "      delta.data = delta.data + self.alpha * delta.grad\n",
        "      #normalize\n",
        "      delta.data =  delta.data / torch.norm(delta.data, p=2, dim=(1, 2, 3), keepdim=True)\n",
        "    delta = delta.detach()\n",
        "\n",
        "    return delta\n",
        "\n",
        "class ProjectedGradientDescentLinf:\n",
        "\n",
        "  def __init__(self, model, eps, alpha, num_iter):\n",
        "    self.model = model\n",
        "    self.eps = eps  # Maximum perturbation\n",
        "    self.alpha = alpha  # Step size\n",
        "    self.num_iter = num_iter  # Number of iterations\n",
        "\n",
        "  def compute(self, x, y):\n",
        "    \"\"\" Construct PGD adversarial perturbation on the examples x.\"\"\"\n",
        "    delta = torch.zeros_like(x, requires_grad=True)  # Initialize delta to zeros\n",
        "\n",
        "    for _ in range(self.num_iter):\n",
        "      # Compute the model output with the current perturbation\n",
        "      output = self.model(x + delta)\n",
        "      # Compute the loss and backpropagate\n",
        "      loss = nn.CrossEntropyLoss()(output, y)\n",
        "      loss.backward()\n",
        "      # Update delta by taking a step in the direction of the gradient\n",
        "      delta.data = delta.data + self.alpha * delta.grad.sign()\n",
        "      # Project delta back into the L_inf ball of radius eps\n",
        "      delta.data = torch.clamp(delta.data, -self.eps, self.eps)\n",
        "      # Zero out gradients before the next iteration\n",
        "      delta.grad.zero_()\n",
        "    # Detach delta to return the final adversarial perturbation\n",
        "    delta = delta.detach()\n",
        "\n",
        "    return delta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJM3XkrFTRHT"
      },
      "outputs": [],
      "source": [
        "def adversarial_train_model(model, criterion, optimizer, loader, epochs=5, attack=None, perc_attacked=1, verbose=True):\n",
        "  \"\"\"Function to train the model\"\"\"\n",
        "\n",
        "  f = 0\n",
        "  train_losses = []\n",
        "  for e in range(epochs):\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        if cuda:\n",
        "          images, labels = images.cuda(), labels.cuda()\n",
        "        f = f + 1\n",
        "        model.train() # - Set the model to train mode\n",
        "        optimizer.zero_grad()# - Reset the optimizer\n",
        "\n",
        "        if attack is None:\n",
        "          output = model(images)\n",
        "        else:\n",
        "          if random() < perc_attacked:\n",
        "            delta = attack.compute(images, labels)\n",
        "            output = model(images + delta)\n",
        "          else:\n",
        "            output = model(images)\n",
        "\n",
        "        loss = criterion(output, labels)# - Compute the loss\n",
        "        loss.backward()# - Backward pass\n",
        "        optimizer.step()# - Update the weights\n",
        "    if verbose:\n",
        "      print('Epoch: {} \\tTraining Loss: {:.6f}'.format(e+1, loss.item()))\n",
        "    train_losses.append(loss.item()) #save loss\n",
        "\n",
        "  return train_losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C3VJ0qtTRHW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "092540b2-156d-4a18-ef59-b50e9c269cee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on testset: 0.6320\n",
            "accuracy on testset with attack: 0.8960\n"
          ]
        }
      ],
      "source": [
        "def eval_model(model, loader, attack=None):\n",
        "  \"\"\"Function to evaluate your model on a specific loader\"\"\"\n",
        "  accuracy = 0.\n",
        "  n_inputs = 0.\n",
        "  for n_batch, (imgs, labels) in enumerate(loader):\n",
        "    if cuda:\n",
        "      imgs, labels = imgs.cuda(), labels.cuda()\n",
        "    if attack is None:\n",
        "      outputs = model(imgs)\n",
        "    else:\n",
        "      delta = attack.compute(imgs, labels)\n",
        "      adv = imgs + delta\n",
        "      outputs = model(adv)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    accuracy += predicted.eq(labels.data).cpu().sum().numpy()\n",
        "    n_inputs += imgs.shape[0]\n",
        "  accuracy /= n_inputs\n",
        "\n",
        "  if attack is None:\n",
        "    print('accuracy on testset: {:.4f}'.format(accuracy))\n",
        "  else:\n",
        "    print('accuracy on testset with attack: {:.4f}'.format(accuracy))\n",
        "\n",
        "\n",
        "attack = FastGradientSignMethod(model, 0.1)\n",
        "eval_model(model, test_loader)\n",
        "eval_model(model, test_loader, attack)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3vrKZ7eTRHZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e8af6ae-6d86-4065-9931-63d7cd766e04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model without attack:\n",
            "Loss: 1.373399257659912\n",
            "Loss: 1.4012207984924316\n",
            "Loss: 0.9337642192840576\n",
            "Loss: 0.838064968585968\n",
            "Loss: 0.671073317527771\n",
            "Model with attack:\n",
            "Loss: 2.031968832015991\n",
            "Loss: 1.2997732162475586\n",
            "Loss: 0.6595293283462524\n",
            "Loss: 0.5677239894866943\n",
            "Loss: 0.5406886339187622\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.031968832015991,\n",
              " 1.2997732162475586,\n",
              " 0.6595293283462524,\n",
              " 0.5677239894866943,\n",
              " 0.5406886339187622]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "print('Model without attack:')\n",
        "# adverserial training with PGD\n",
        "model1 = ConvModel()\n",
        "if cuda:\n",
        "  model1 = model1.cuda()\n",
        "# define your loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# define the optimizer\n",
        "opt = optim.Adam(model1.parameters(), lr=0.001)\n",
        "adversarial_train_model(model1, criterion, opt, train_loader, None)\n",
        "\n",
        "\n",
        "print('Model with attack:')\n",
        "model2 = ConvModel()\n",
        "if cuda:\n",
        "  model2 = model2.cuda()\n",
        "# define your loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# define the optimizer\n",
        "opt = optim.Adam(model2.parameters(), lr=0.001)\n",
        "# define the attack\n",
        "attack = FastGradientSignMethod(model2, 0.1)\n",
        "adversarial_train_model(model2, criterion, opt, train_loader, attack)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqDnEtU9TRHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46349e48-94df-4751-904e-9fef180fe2ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on testset: 0.7195\n",
            "accuracy on testset with attack: 0.0847\n"
          ]
        }
      ],
      "source": [
        "eval_model(model1, test_loader)\n",
        "eval_model(model1, test_loader, attack)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model(model2, test_loader, attack)\n",
        "eval_model(model2, test_loader, None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBUveM97Lycn",
        "outputId": "e266a965-0095-4ccf-a752-de28800abd77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on testset: 0.6388\n",
            "accuracy on testset with attack: 0.8166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TbCZarb3L0vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PLAN  - Adversarial Attacks on CIFAR-10: Robustness and Tradeoffs\n",
        "\n",
        "## 1. Initial Analysis: Training Robust Models and Accuracy Tradeoff\n",
        "\n",
        "### A. Training Strategies for Robustness\n",
        "- **Standard Training**\n",
        "  - Description of the ConvModel architecture\n",
        "  - Training on clean CIFAR-10 dataset\n",
        "- **Adversarial Training**\n",
        "  - Incorporation of adversarial examples (FGSM, PGD, PGD\\_l2) during training\n",
        "  - Training process and parameters\n",
        "- **Defense Mechanisms**\n",
        "  - Implementation of additional defenses (e.g., gradient masking, regularization techniques)\n",
        "  - Evaluation of their effectiveness\n",
        "\n",
        "### B. Evaluating Robustness vs. Accuracy Tradeoff\n",
        "- **Metrics to Consider**\n",
        "  - Clean Accuracy: Performance on unaltered test set\n",
        "  - Robust Accuracy: Performance on adversarially perturbed test set\n",
        "- **Tradeoff Insights**\n",
        "  - Analysis of how adversarial training affects clean and robust accuracy\n",
        "  - Comparison of different adversarial training methods\n",
        "- **Visualization**\n",
        "  - Graphs plotting clean vs. robust accuracy for various training strategies\n",
        "- **Statistical Analysis**\n",
        "  - Statistical significance of observed tradeoffs\n",
        "  - Discussion on reasons behind the tradeoff\n",
        "\n",
        "## 2. Elaboration Topic: Transferability of Adversarial Attacks\n",
        "\n",
        "### A. Understanding Transferability\n",
        "- **Definition**\n",
        "  - Explanation of transferability in the context of adversarial attacks\n",
        "- **Mechanisms**\n",
        "  - How adversarial examples crafted for one model can affect others\n",
        "\n",
        "### B. Importance of Studying Transferability\n",
        "- **Black-Box Attacks**\n",
        "  - Utilization of transferability for attacks without access to target model parameters\n",
        "- **Robustness Implications**\n",
        "  - Impact on the general robustness of different models\n",
        "- **Security Considerations**\n",
        "  - Implications for the security of deployed machine learning systems\n",
        "\n",
        "### C. Experimental Approach\n",
        "- **Multiple Models**\n",
        "  - Training diverse ConvModels with varying architectures and hyperparameters\n",
        "  - Inclusion of models with different adversarial training methods\n",
        "- **Generating Adversarial Examples**\n",
        "  - Creation of adversarial examples using FGSM, PGD, and PGD\\_l2 on source models\n",
        "  - Testing these examples on target models to assess transferability\n",
        "- **Metrics**\n",
        "  - Transfer Success Rate: Percentage of adversarial examples deceiving target models\n",
        "  - Robust Accuracy Across Models: Measuring general robustness\n",
        "- **Factors Influencing Transferability**\n",
        "  - Model Similarity: Impact of architectural similarities\n",
        "  - Training Methods: Effect of different defense strategies\n",
        "  - Attack Strength: Role of perturbation magnitude in transferability"
      ],
      "metadata": {
        "id": "T7vD66pdz6B0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adversarial Attacks on CIFAR-10: Robustness and Tradeoffs\n"
      ],
      "metadata": {
        "id": "6ZJO_8vu2Fxu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Initial Analysis: Training Robust Models and Accuracy Tradeoff\n"
      ],
      "metadata": {
        "id": "gvPD6npS2IWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A. Training Strategies for Robustness"
      ],
      "metadata": {
        "id": "beqO5vr01-QZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Standard Training**"
      ],
      "metadata": {
        "id": "umFGYdJt2RoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Model without attack:')\n",
        "# adverserial training with PGD\n",
        "model_noattack = ConvModel()\n",
        "if cuda:\n",
        "  model_noattack = model_noattack.cuda()\n",
        "# define your loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# define the optimizer\n",
        "opt = optim.Adam(model_noattack.parameters(), lr=0.001)\n",
        "#define attack\n",
        "attack = None\n",
        "#define % attacked of the train dataset\n",
        "perc_attacked = 1\n",
        "# define epoch\n",
        "epochs = 20\n",
        "\n",
        "train_losses = adversarial_train_model(model_noattack, criterion, opt, train_loader, epochs, attack, perc_attacked, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc9Yq2gOz9BD",
        "outputId": "71991c9b-b767-44ae-dcb8-8a950d6880a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model without attack:\n",
            "Epoch: 1 \tTraining Loss: 1.220773\n",
            "Epoch: 2 \tTraining Loss: 1.208490\n",
            "Epoch: 3 \tTraining Loss: 0.930145\n",
            "Epoch: 4 \tTraining Loss: 0.888815\n",
            "Epoch: 5 \tTraining Loss: 0.688768\n",
            "Epoch: 6 \tTraining Loss: 0.787551\n",
            "Epoch: 7 \tTraining Loss: 0.650179\n",
            "Epoch: 8 \tTraining Loss: 0.621199\n",
            "Epoch: 9 \tTraining Loss: 0.622477\n",
            "Epoch: 10 \tTraining Loss: 0.509969\n",
            "Epoch: 11 \tTraining Loss: 0.527332\n",
            "Epoch: 12 \tTraining Loss: 0.458468\n",
            "Epoch: 13 \tTraining Loss: 0.352289\n",
            "Epoch: 14 \tTraining Loss: 0.345791\n",
            "Epoch: 15 \tTraining Loss: 0.549821\n",
            "Epoch: 16 \tTraining Loss: 0.326741\n",
            "Epoch: 17 \tTraining Loss: 0.381404\n",
            "Epoch: 18 \tTraining Loss: 0.274787\n",
            "Epoch: 19 \tTraining Loss: 0.315009\n",
            "Epoch: 20 \tTraining Loss: 0.193689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Adversarial Training**\n",
        "\n",
        "1. FGSM"
      ],
      "metadata": {
        "id": "2dOzHnjv4XOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Model with attack FGSM:')\n",
        "# adverserial training with PGD\n",
        "model_withattackFGSM = ConvModel()\n",
        "if cuda:\n",
        "  model_withattack = model_withattackFGSM.cuda()\n",
        "# define your loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# define the optimizer\n",
        "opt = optim.Adam(model_withattackFGSM.parameters(), lr=0.001)\n",
        "#define attack\n",
        "eps = 0.1\n",
        "fgsm = FastGradientSignMethod(model_withattackFGSM, eps)\n",
        "#define % attacked of the train dataset\n",
        "perc_attacked = 1\n",
        "# define epoch\n",
        "epochs = 20\n",
        "\n",
        "train_losses = adversarial_train_model(model_withattackFGSM, criterion, opt, train_loader, epochs, attack=fgsm, perc_attacked=perc_attacked, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "9bX6Gx6H4NZR",
        "outputId": "6ab37ebf-664a-44ed-fa67-7112c354b7f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model with attack FGSM:\n",
            "Epoch: 1 \tTraining Loss: 2.104448\n",
            "Epoch: 2 \tTraining Loss: 1.028874\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-3f413a879de0>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtrain_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madversarial_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_withattackFGSM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfgsm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperc_attacked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mperc_attacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-776962b40ae8>\u001b[0m in \u001b[0;36madversarial_train_model\u001b[0;34m(model, criterion, optimizer, loader, epochs, attack, perc_attacked, verbose)\u001b[0m\n\u001b[1;32m      9\u001b[0m           \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# - Set the model to train mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# - Reset the optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2453\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training mode is expected to be boolean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2454\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2455\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1738\u001b[0m                         \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1740\u001b[0;31m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. PGB L0"
      ],
      "metadata": {
        "id": "pSQ1O3WH7lku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Model with attack PGD L0:')\n",
        "# adverserial training with PGD\n",
        "model_withattackPGD = ConvModel()\n",
        "if cuda:\n",
        "  model_withattack = model_withattackPGD.cuda()\n",
        "# define your loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# define the optimizer\n",
        "opt = optim.Adam(model_withattackPGD.parameters(), lr=0.001)\n",
        "#define attack\n",
        "eps = 0.1\n",
        "pdg_l0 = ProjectedGradientDescent(model_withattackPGD, 0.1, 0.01, 10)\n",
        "#define % attacked of the train dataset\n",
        "perc_attacked = 1\n",
        "# define epoch\n",
        "epochs = 20\n",
        "\n",
        "train_losses = adversarial_train_model(model_withattackPGD, criterion, opt, train_loader, epochs, attack=pdg_l0, perc_attacked=perc_attacked, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSABc4wa7zjf",
        "outputId": "4797b46c-05fc-41d2-ca6a-c09945025ad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model with attack PGD L0:\n",
            "Epoch: 1 \tTraining Loss: 2.363302\n",
            "Epoch: 2 \tTraining Loss: 2.344585\n",
            "Epoch: 3 \tTraining Loss: 2.426562\n",
            "Epoch: 4 \tTraining Loss: 2.352633\n",
            "Epoch: 5 \tTraining Loss: 2.374098\n",
            "Epoch: 6 \tTraining Loss: 2.421401\n",
            "Epoch: 7 \tTraining Loss: 2.363866\n",
            "Epoch: 8 \tTraining Loss: 2.355221\n",
            "Epoch: 9 \tTraining Loss: 2.335323\n",
            "Epoch: 10 \tTraining Loss: 2.377704\n",
            "Epoch: 11 \tTraining Loss: 2.407084\n",
            "Epoch: 12 \tTraining Loss: 2.328311\n",
            "Epoch: 13 \tTraining Loss: 2.343179\n",
            "Epoch: 14 \tTraining Loss: 2.382799\n",
            "Epoch: 15 \tTraining Loss: 2.402846\n",
            "Epoch: 16 \tTraining Loss: 2.465587\n",
            "Epoch: 17 \tTraining Loss: 2.433835\n",
            "Epoch: 18 \tTraining Loss: 2.364945\n",
            "Epoch: 19 \tTraining Loss: 2.214850\n",
            "Epoch: 20 \tTraining Loss: 2.151494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. PGD L2"
      ],
      "metadata": {
        "id": "KRso6YIU8nVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Model with attack PGD L2:')\n",
        "# adverserial training with PGD\n",
        "model_withattackPGDL2 = ConvModel()\n",
        "if cuda:\n",
        "  model_withattack = model_withattackPGDL2.cuda()\n",
        "# define your loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# define the optimizer\n",
        "opt = optim.Adam(model_withattackPGDL2.parameters(), lr=0.001)\n",
        "#define attack\n",
        "eps = 0.1\n",
        "pdg_l2 = ProjectedGradientDescent(model_withattackPGDL2, 0.1, 0.01, 10)\n",
        "#define % attacked of the train dataset\n",
        "perc_attacked = 1\n",
        "# define epoch\n",
        "epochs = 20\n",
        "\n",
        "train_losses = adversarial_train_model(model_withattackPGDL2, criterion, opt, train_loader, epochs, attack=pdg_l2, perc_attacked=perc_attacked, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "P1WaZbVp8okJ",
        "outputId": "21f7cc5b-3963-497e-bedd-c5e37c447d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model with attack PGD L2:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-0f10d251ba2a>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtrain_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madversarial_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_withattackPGDL2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpdg_l2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperc_attacked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mperc_attacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-776962b40ae8>\u001b[0m in \u001b[0;36madversarial_train_model\u001b[0;34m(model, criterion, optimizer, loader, epochs, attack, perc_attacked, verbose)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mperc_attacked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-44515d0c27a9>\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m       \u001b[0mdelta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0mdelta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    770\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. PGD Linf\n"
      ],
      "metadata": {
        "id": "Bipq8jR886Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Model with attack PGD Linf:')\n",
        "# adverserial training with PGD\n",
        "model_withattackPGDLinf = ConvModel()\n",
        "if cuda:\n",
        "  model_withattack = model_withattackPGDLinf.cuda()\n",
        "# define your loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# define the optimizer\n",
        "opt = optim.Adam(model_withattackPGDLinf.parameters(), lr=0.001)\n",
        "#define attack\n",
        "eps = 0.1\n",
        "pdg_linf = ProjectedGradientDescent(model_withattackPGDLinf, 0.1, 0.01, 5)\n",
        "#define % attacked of the train dataset\n",
        "perc_attacked = 1\n",
        "# define epoch\n",
        "epochs = 20\n",
        "\n",
        "train_losses = adversarial_train_model(model_withattackPGDLinf, criterion, opt, train_loader, epochs, attack=pdg_linf, perc_attacked=perc_attacked, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJDF60_K875b",
        "outputId": "95d20322-cf10-43f1-b540-1f87024fbc0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model with attack PGD Linf:\n",
            "Epoch: 1 \tTraining Loss: 1.318361\n",
            "Epoch: 2 \tTraining Loss: 1.215485\n",
            "Epoch: 3 \tTraining Loss: 0.973842\n",
            "Epoch: 4 \tTraining Loss: 0.994524\n",
            "Epoch: 5 \tTraining Loss: 1.066445\n",
            "Epoch: 6 \tTraining Loss: 1.026074\n",
            "Epoch: 7 \tTraining Loss: 1.007287\n",
            "Epoch: 8 \tTraining Loss: 0.888043\n",
            "Epoch: 9 \tTraining Loss: 0.828196\n",
            "Epoch: 10 \tTraining Loss: 0.737218\n",
            "Epoch: 11 \tTraining Loss: 0.644678\n",
            "Epoch: 12 \tTraining Loss: 0.717615\n",
            "Epoch: 13 \tTraining Loss: 0.544334\n",
            "Epoch: 14 \tTraining Loss: 0.591962\n",
            "Epoch: 15 \tTraining Loss: 0.606081\n",
            "Epoch: 16 \tTraining Loss: 0.743542\n",
            "Epoch: 17 \tTraining Loss: 0.779675\n",
            "Epoch: 18 \tTraining Loss: 0.464911\n",
            "Epoch: 19 \tTraining Loss: 0.426206\n",
            "Epoch: 20 \tTraining Loss: 0.323298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B. Evaluating Robustness vs. Accuracy Tradeoff"
      ],
      "metadata": {
        "id": "mccgtYXK9VQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metrics to Consider**"
      ],
      "metadata": {
        "id": "V6n98afQ9YmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, loader, attack=None, attack_name='', model_name=''):\n",
        "  \"\"\"Function to evaluate your model on a specific loader\"\"\"\n",
        "  accuracy = 0.\n",
        "  n_inputs = 0.\n",
        "  for n_batch, (imgs, labels) in enumerate(loader):\n",
        "    if cuda:\n",
        "      imgs, labels = imgs.cuda(), labels.cuda()\n",
        "    if attack is None:\n",
        "      outputs = model(imgs)\n",
        "    else:\n",
        "      delta = attack.compute(imgs, labels)\n",
        "      adv = imgs + delta\n",
        "      outputs = model(adv)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    accuracy += predicted.eq(labels.data).cpu().sum().numpy()\n",
        "    n_inputs += imgs.shape[0]\n",
        "  accuracy /= n_inputs\n",
        "\n",
        "  if attack is None:\n",
        "    print('Clean Accuracy, model {} on testset {}: {:.4f}'.format(model_name, attack_name, accuracy))\n",
        "  else:\n",
        "    print('Robust Accuracy, model {} on testset with attack {}: {:.4f}'.format(model_name, attack_name, accuracy))\n"
      ],
      "metadata": {
        "id": "6M6Yfz2G9Xpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tradeoff Insights**\n"
      ],
      "metadata": {
        "id": "dChiHpND9qaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Baseline Clean Accuracy\n",
        "eval_model(model_noattack, test_loader, attack=None, attack_name='', model_name='baseline')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZUddgahD91W",
        "outputId": "d275bec4-381f-4fe8-fd03-59d735a17b05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean Accuracy, model baseline on testset : 0.9568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Robust Accuracy: eval model attacked test set\n",
        "eval_model(model_noattack, test_loader, attack=fgsm, attack_name='FGSM', model_name='baseline')\n",
        "eval_model(model_noattack, test_loader, attack=pdg_l0, attack_name='PGD_L0', model_name='baseline')\n",
        "eval_model(model_noattack, test_loader, attack=pdg_l2, attack_name='PGD_L2', model_name='baseline')\n",
        "eval_model(model_noattack, test_loader, attack=pdg_linf, attack_name='PGD_Linf',model_name='baseline')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqpQ6tEb9psr",
        "outputId": "1cf93af2-7e97-4246-e475-c2ac2bbae908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Robust Accuracy, model baseline on testset with attack FGSM: 0.2108\n",
            "Robust Accuracy, model baseline on testset with attack PGD_L0: 0.3330\n",
            "Robust Accuracy, model baseline on testset with attack PGD_L2: 0.7860\n",
            "Robust Accuracy, model baseline on testset with attack PGD_Linf: 0.9422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean Accuracy: eval model on not attacked test set\n",
        "eval_model(model_withattackFGSM, test_loader, attack=None, attack_name='', model_name='FGSM')\n",
        "eval_model(model_withattackPGD, test_loader, attack=None, attack_name='', model_name='PGD_L0')\n",
        "eval_model(model_withattackPGDL2, test_loader, attack=None, attack_name='', model_name='PGD_L2')\n",
        "eval_model(model_withattackPGDLinf, test_loader, attack=None, attack_name='', model_name='PGD_Linf')\n",
        "\n",
        "#Robust Accuracy: eval model attacked test set\n",
        "eval_model(model_withattackFGSM, test_loader, attack=fgsm, attack_name='FGSM', model_name='FGSM')\n",
        "eval_model(model_withattackPGD, test_loader, attack=pdg_l0, attack_name='PGD_L0', model_name='PGD_L0')\n",
        "eval_model(model_withattackPGDL2, test_loader, attack=pdg_l2, attack_name='PGD_L2', model_name='PGD_L2')\n",
        "eval_model(model_withattackPGDLinf, test_loader, attack=pdg_linf, attack_name='PGD_Linf', model_name='PGD_Linf')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmASHeVE_YVN",
        "outputId": "ad02161e-1e4d-43f4-ff51-8018bdcbec25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean Accuracy, model FGSM on testset : 0.5326\n",
            "Clean Accuracy, model PGD_L0 on testset : 0.4327\n",
            "Clean Accuracy, model PGD_L2 on testset : 0.1353\n",
            "Clean Accuracy, model PGD_Linf on testset : 0.9434\n",
            "Robust Accuracy, model FGSM on testset with attack FGSM: 0.6686\n",
            "Robust Accuracy, model PGD_L0 on testset with attack PGD_L0: 0.1165\n",
            "Robust Accuracy, model PGD_L2 on testset with attack PGD_L2: 0.0890\n",
            "Robust Accuracy, model PGD_Linf on testset with attack PGD_Linf: 0.8491\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}