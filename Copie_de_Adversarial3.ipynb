{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbenayoun/Generative-AI-Module-Dauphine/blob/main/Copie_de_Adversarial3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "YroMaxFATRHG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from random import random\n",
        "\n",
        "cuda = torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiY3BiAiTRHK"
      },
      "source": [
        "## From adversarial examples to training robust models\n",
        "\n",
        "In the previous notebooks, we focused on methods for solving the maximization problem over perturbations; that is, to finding the solution to the problem\n",
        "\\begin{equation}\n",
        "\\DeclareMathOperator*{\\maximize}{maximize}\n",
        "\\maximize_{\\|\\delta\\| \\leq \\epsilon} \\ell(h_\\theta(x + \\delta), y).\n",
        "\\end{equation}\n",
        "\n",
        "In this notebook, we will focus on training a robust classifier. More precisly, we aim at solving following minimization problem, namely Adversarial Training:\n",
        "\\begin{equation}\n",
        "\\DeclareMathOperator*{\\minimize}{minimize}\n",
        "\\minimize_\\theta \\frac{1}{|S|} \\sum_{x,y \\in S} \\max_{\\|\\delta\\| \\leq \\epsilon} \\ell(h_\\theta(x + \\delta), y).\n",
        "\\end{equation}\n",
        "The order of the min-max operations is important here.  Specially, the max is inside the minimization, meaning that the adversary (trying to maximize the loss) gets to \"move\" _second_.  We assume, essentially, that the adversary has full knowledge of the classifier parameters $\\theta$, and that they get to specialize their attack to whatever parameters we have chosen in the outer minimization. The goal of the robust optimization formulation, therefore, is to ensure that the model cannot be attacked _even if_ the adversary has full knowledge of the model.  Of course, in practice we may want to make assumptions about the power of the adversary but it can be difficult to pin down a precise definition of what we mean by the \"power\" of the adversary, so extra care should be taken in evaluating models against possible \"realistic\" adversaries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QMXb08jTRHL"
      },
      "source": [
        "## Exercice 1\n",
        "1. Train a robust classifier using Adversarial Training with a specific norm\n",
        "2. Evaluate your classifier on natural and adversarial examples crafted with the norm of the training and other norms\n",
        "3. Make an analysis and conclude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "w21alYjaTRHL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "670a71e7-97d6-49fb-aca1-bf766338b2d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./docs/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 44278943.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./docs/cifar-10-python.tar.gz to ./docs\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# load CIFAR10 dataset\n",
        "def load_cifar(split, batch_size):\n",
        "  train = True if split == 'train' else False\n",
        "  dataset = datasets.CIFAR10(\"./docs\", train=split, download=True, transform=transforms.ToTensor())\n",
        "  return DataLoader(dataset, batch_size=batch_size, shuffle=train)\n",
        "\n",
        "batch_size = 100\n",
        "train_loader = load_cifar('train', batch_size)\n",
        "test_loader = load_cifar('test', batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "e5awbg2Qst1h"
      },
      "outputs": [],
      "source": [
        "class ConvModel(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(ConvModel, self).__init__()\n",
        "    # First Convolutional Layer: Input channels = 3, Output channels = 32\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "    # First Max Pooling Layer: Reduces each dimension by a factor of 2\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "    # Second Convolutional Layer: Input channels = 32, Output channels = 64\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "    # Second Max Pooling Layer: Reduces each dimension by a factor of 2\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "    self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, 10)  # Assuming 10 output classes\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # First convolutional block: Conv -> ReLU -> MaxPool\n",
        "    x = self.pool1(nn.functional.relu(self.conv1(x)))\n",
        "    # Second convolutional block: Conv -> ReLU -> MaxPool\n",
        "    x = self.pool2(nn.functional.relu(self.conv2(x)))\n",
        "\n",
        "    # Flatten the output for the fully connected layers\n",
        "    x = x.view(-1, 64 * 8 * 8)  # Flattening the output from conv layers\n",
        "\n",
        "    # First fully connected block: FC -> ReLU\n",
        "    x = nn.functional.relu(self.fc1(x))\n",
        "    # Second fully connected block: FC -> ReLU\n",
        "    x = nn.functional.relu(self.fc2(x))\n",
        "    # Output layer: FC\n",
        "    x = self.fc3(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "j9B-ea_dTRHO"
      },
      "outputs": [],
      "source": [
        "class FastGradientSignMethod:\n",
        "\n",
        "  def __init__(self, model, eps):\n",
        "    self.model = model\n",
        "    self.eps = eps\n",
        "\n",
        "\n",
        "  def compute(self, x, y):\n",
        "    \"\"\" Construct FGSM adversarial perturbation for examples x\"\"\"\n",
        "    delta = torch.zeros_like(x, requires_grad=True)\n",
        "    # code here ...\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    output = self.model(x + delta)\n",
        "    loss = criterion(output, y)\n",
        "    loss.backward()\n",
        "    # Use variable.grad.detach() to retreive the gradient with respect to a loss\n",
        "    grad = delta.grad.detach()\n",
        "    delta = self.eps * grad.sign()\n",
        "\n",
        "    return delta\n",
        "\n",
        "class ProjectedGradientDescent:\n",
        "\n",
        "  def __init__(self, model, eps, alpha, num_iter):\n",
        "    self.model = model\n",
        "    self.eps = eps\n",
        "    self.alpha = alpha\n",
        "    self.num_iter = num_iter\n",
        "\n",
        "  def compute(self, x, y):\n",
        "    \"\"\" Construct PGD adversarial pertubration on the examples x.\"\"\"\n",
        "    delta = torch.zeros_like(x, requires_grad=True)\n",
        "    for _ in range(self.num_iter):\n",
        "      output = self.model(x + delta)\n",
        "      loss = nn.CrossEntropyLoss()(output, y)\n",
        "      loss.backward()\n",
        "      delta.data = delta.data + self.alpha * delta.grad.sign()\n",
        "      delta.data = torch.clamp(delta.data, -self.eps, self.eps)\n",
        "    delta = delta.detach()\n",
        "\n",
        "    return delta\n",
        "\n",
        "class ProjectedGradientDescentl2:\n",
        "\n",
        "  def __init__(self, model, eps, alpha, num_iter):\n",
        "    # code here ...\n",
        "    self.model = model\n",
        "    self.eps = eps\n",
        "    self.alpha = alpha\n",
        "    self.num_iter = num_iter\n",
        "\n",
        "  def compute(self, x, y):\n",
        "    \"\"\" Construct PGD adversarial pertubration on the examples x.\"\"\"\n",
        "    # code here ...\n",
        "    delta = torch.zeros_like(x, requires_grad=True)\n",
        "    for _ in range(self.num_iter):\n",
        "      output = self.model(x + delta)\n",
        "      loss = nn.CrossEntropyLoss()(output, y)\n",
        "      loss.backward()\n",
        "      delta.data = delta.data + self.alpha * delta.grad\n",
        "      #normalize\n",
        "      delta.data =  delta.data / torch.norm(delta.data, p=2, dim=(1, 2, 3), keepdim=True)\n",
        "    delta = delta.detach()\n",
        "\n",
        "    return delta\n",
        "\n",
        "class ProjectedGradientDescentLinf:\n",
        "\n",
        "  def __init__(self, model, eps, alpha, num_iter):\n",
        "    self.model = model\n",
        "    self.eps = eps  # Maximum perturbation\n",
        "    self.alpha = alpha  # Step size\n",
        "    self.num_iter = num_iter  # Number of iterations\n",
        "\n",
        "  def compute(self, x, y):\n",
        "    \"\"\" Construct PGD adversarial perturbation on the examples x.\"\"\"\n",
        "    delta = torch.zeros_like(x, requires_grad=True)  # Initialize delta to zeros\n",
        "\n",
        "    for _ in range(self.num_iter):\n",
        "      # Compute the model output with the current perturbation\n",
        "      output = self.model(x + delta)\n",
        "      # Compute the loss and backpropagate\n",
        "      loss = nn.CrossEntropyLoss()(output, y)\n",
        "      loss.backward()\n",
        "      # Update delta by taking a step in the direction of the gradient\n",
        "      delta.data = delta.data + self.alpha * delta.grad.sign()\n",
        "      # Project delta back into the L_inf ball of radius eps\n",
        "      delta.data = torch.clamp(delta.data, -self.eps, self.eps)\n",
        "      # Zero out gradients before the next iteration\n",
        "      delta.grad.zero_()\n",
        "    # Detach delta to return the final adversarial perturbation\n",
        "    delta = delta.detach()\n",
        "\n",
        "    return delta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ZJM3XkrFTRHT"
      },
      "outputs": [],
      "source": [
        "def adversarial_train_model(model, criterion, optimizer, loader, epochs=5, attack=None, perc_attacked=1, verbose=True):\n",
        "  \"\"\"Function to train the model\"\"\"\n",
        "\n",
        "  f = 0\n",
        "  train_losses = []\n",
        "  for e in range(epochs):\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        if cuda:\n",
        "          images, labels = images.cuda(), labels.cuda()\n",
        "        f = f + 1\n",
        "        model.train() # - Set the model to train mode\n",
        "        optimizer.zero_grad()# - Reset the optimizer\n",
        "\n",
        "        if attack is None:\n",
        "          output = model(images)\n",
        "        else:\n",
        "          if random() < perc_attacked:\n",
        "            delta = attack.compute(images, labels)\n",
        "            output = model(images + delta)\n",
        "          else:\n",
        "            output = model(images)\n",
        "\n",
        "        loss = criterion(output, labels)# - Compute the loss\n",
        "        loss.backward()# - Backward pass\n",
        "        optimizer.step()# - Update the weights\n",
        "    if verbose:\n",
        "      print('Epoch: {} \\tTraining Loss: {:.6f}'.format(e+1, loss.item()))\n",
        "    train_losses.append(loss.item()) #save loss\n",
        "\n",
        "  return train_losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2C3VJ0qtTRHW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "092540b2-156d-4a18-ef59-b50e9c269cee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on testset: 0.6320\n",
            "accuracy on testset with attack: 0.8960\n"
          ]
        }
      ],
      "source": [
        "def eval_model(model, loader, attack=None):\n",
        "  \"\"\"Function to evaluate your model on a specific loader\"\"\"\n",
        "  accuracy = 0.\n",
        "  n_inputs = 0.\n",
        "  for n_batch, (imgs, labels) in enumerate(loader):\n",
        "    if cuda:\n",
        "      imgs, labels = imgs.cuda(), labels.cuda()\n",
        "    if attack is None:\n",
        "      outputs = model(imgs)\n",
        "    else:\n",
        "      delta = attack.compute(imgs, labels)\n",
        "      adv = imgs + delta\n",
        "      outputs = model(adv)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    accuracy += predicted.eq(labels.data).cpu().sum().numpy()\n",
        "    n_inputs += imgs.shape[0]\n",
        "  accuracy /= n_inputs\n",
        "\n",
        "  if attack is None:\n",
        "    print('accuracy on testset: {:.4f}'.format(accuracy))\n",
        "  else:\n",
        "    print('accuracy on testset with attack: {:.4f}'.format(accuracy))\n",
        "\n",
        "\n",
        "attack = FastGradientSignMethod(model, 0.1)\n",
        "eval_model(model, test_loader)\n",
        "eval_model(model, test_loader, attack)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "N3vrKZ7eTRHZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e8af6ae-6d86-4065-9931-63d7cd766e04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model without attack:\n",
            "Loss: 1.373399257659912\n",
            "Loss: 1.4012207984924316\n",
            "Loss: 0.9337642192840576\n",
            "Loss: 0.838064968585968\n",
            "Loss: 0.671073317527771\n",
            "Model with attack:\n",
            "Loss: 2.031968832015991\n",
            "Loss: 1.2997732162475586\n",
            "Loss: 0.6595293283462524\n",
            "Loss: 0.5677239894866943\n",
            "Loss: 0.5406886339187622\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.031968832015991,\n",
              " 1.2997732162475586,\n",
              " 0.6595293283462524,\n",
              " 0.5677239894866943,\n",
              " 0.5406886339187622]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "print('Model without attack:')\n",
        "# adverserial training with PGD\n",
        "model1 = ConvModel()\n",
        "if cuda:\n",
        "  model1 = model1.cuda()\n",
        "# define your loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# define the optimizer\n",
        "opt = optim.Adam(model1.parameters(), lr=0.001)\n",
        "adversarial_train_model(model1, criterion, opt, train_loader, None)\n",
        "\n",
        "\n",
        "print('Model with attack:')\n",
        "model2 = ConvModel()\n",
        "if cuda:\n",
        "  model2 = model2.cuda()\n",
        "# define your loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# define the optimizer\n",
        "opt = optim.Adam(model2.parameters(), lr=0.001)\n",
        "# define the attack\n",
        "attack = FastGradientSignMethod(model2, 0.1)\n",
        "adversarial_train_model(model2, criterion, opt, train_loader, attack)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zqDnEtU9TRHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46349e48-94df-4751-904e-9fef180fe2ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on testset: 0.7195\n",
            "accuracy on testset with attack: 0.0847\n"
          ]
        }
      ],
      "source": [
        "eval_model(model1, test_loader)\n",
        "eval_model(model1, test_loader, attack)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model(model2, test_loader, attack)\n",
        "eval_model(model2, test_loader, None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBUveM97Lycn",
        "outputId": "e266a965-0095-4ccf-a752-de28800abd77"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on testset: 0.6388\n",
            "accuracy on testset with attack: 0.8166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TbCZarb3L0vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PLAN  - Adversarial Attacks on CIFAR-10: Robustness and Tradeoffs\n",
        "\n",
        "## 1. Initial Analysis: Training Robust Models and Accuracy Tradeoff\n",
        "\n",
        "### A. Training Strategies for Robustness\n",
        "- **Standard Training**\n",
        "  - Description of the ConvModel architecture\n",
        "  - Training on clean CIFAR-10 dataset\n",
        "- **Adversarial Training**\n",
        "  - Incorporation of adversarial examples (FGSM, PGD, PGD\\_l2) during training\n",
        "  - Training process and parameters\n",
        "- **Defense Mechanisms**\n",
        "  - Implementation of additional defenses (e.g., gradient masking, regularization techniques)\n",
        "  - Evaluation of their effectiveness\n",
        "\n",
        "### B. Evaluating Robustness vs. Accuracy Tradeoff\n",
        "- **Metrics to Consider**\n",
        "  - Clean Accuracy: Performance on unaltered test set\n",
        "  - Robust Accuracy: Performance on adversarially perturbed test set\n",
        "- **Tradeoff Insights**\n",
        "  - Analysis of how adversarial training affects clean and robust accuracy\n",
        "  - Comparison of different adversarial training methods\n",
        "- **Visualization**\n",
        "  - Graphs plotting clean vs. robust accuracy for various training strategies\n",
        "- **Statistical Analysis**\n",
        "  - Statistical significance of observed tradeoffs\n",
        "  - Discussion on reasons behind the tradeoff\n",
        "\n",
        "## 2. Elaboration Topic: Transferability of Adversarial Attacks\n",
        "\n",
        "### A. Understanding Transferability\n",
        "- **Definition**\n",
        "  - Explanation of transferability in the context of adversarial attacks\n",
        "- **Mechanisms**\n",
        "  - How adversarial examples crafted for one model can affect others\n",
        "\n",
        "### B. Importance of Studying Transferability\n",
        "- **Black-Box Attacks**\n",
        "  - Utilization of transferability for attacks without access to target model parameters\n",
        "- **Robustness Implications**\n",
        "  - Impact on the general robustness of different models\n",
        "- **Security Considerations**\n",
        "  - Implications for the security of deployed machine learning systems\n",
        "\n",
        "### C. Experimental Approach\n",
        "- **Multiple Models**\n",
        "  - Training diverse ConvModels with varying architectures and hyperparameters\n",
        "  - Inclusion of models with different adversarial training methods\n",
        "- **Generating Adversarial Examples**\n",
        "  - Creation of adversarial examples using FGSM, PGD, and PGD\\_l2 on source models\n",
        "  - Testing these examples on target models to assess transferability\n",
        "- **Metrics**\n",
        "  - Transfer Success Rate: Percentage of adversarial examples deceiving target models\n",
        "  - Robust Accuracy Across Models: Measuring general robustness\n",
        "- **Factors Influencing Transferability**\n",
        "  - Model Similarity: Impact of architectural similarities\n",
        "  - Training Methods: Effect of different defense strategies\n",
        "  - Attack Strength: Role of perturbation magnitude in transferability"
      ],
      "metadata": {
        "id": "T7vD66pdz6B0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adversarial Attacks on CIFAR-10: Robustness and Tradeoffs\n"
      ],
      "metadata": {
        "id": "6ZJO_8vu2Fxu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Initial Analysis: Training Robust Models and Accuracy Tradeoff\n"
      ],
      "metadata": {
        "id": "gvPD6npS2IWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A. Training Strategies for Robustness"
      ],
      "metadata": {
        "id": "beqO5vr01-QZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Standard Training**"
      ],
      "metadata": {
        "id": "umFGYdJt2RoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Model without attack:')\n",
        "# adverserial training with PGD\n",
        "model_noattack = ConvModel()\n",
        "if cuda:\n",
        "  model_noattack = model_noattack.cuda()\n",
        "# define your loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# define the optimizer\n",
        "opt = optim.Adam(model_noattack.parameters(), lr=0.001)\n",
        "#define attack\n",
        "attack = None\n",
        "#define % attacked of the train dataset\n",
        "perc_attacked = 1\n",
        "# define epoch\n",
        "epochs = 5\n",
        "\n",
        "train_losses = adversarial_train_model(model_noattack, criterion, opt, train_loader, epochs, attack, perc_attacked, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc9Yq2gOz9BD",
        "outputId": "b8332dc9-a343-489e-e72d-a7071a356515"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model without attack:\n",
            "Epoch: 1 \tTraining Loss: 1.251857\n",
            "Epoch: 2 \tTraining Loss: 1.137900\n",
            "Epoch: 3 \tTraining Loss: 1.014634\n",
            "Epoch: 4 \tTraining Loss: 1.040884\n",
            "Epoch: 5 \tTraining Loss: 1.002644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Adversarial Training**\n",
        "\n",
        "1. FGSM"
      ],
      "metadata": {
        "id": "2dOzHnjv4XOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Model with attack FGSM:')\n",
        "# adverserial training with PGD\n",
        "model_withattackFGSM = ConvModel()\n",
        "if cuda:\n",
        "  model_withattack = model_withattackFGSM.cuda()\n",
        "# define your loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# define the optimizer\n",
        "opt = optim.Adam(model_withattackFGSM.parameters(), lr=0.001)\n",
        "#define attack\n",
        "eps = 0.1\n",
        "fgsm = FastGradientSignMethod(model_withattackFGSM, eps)\n",
        "#define % attacked of the train dataset\n",
        "perc_attacked = 1\n",
        "# define epoch\n",
        "epochs = 5\n",
        "\n",
        "train_losses = adversarial_train_model(model_withattackFGSM, criterion, opt, train_loader, epochs, attack=fgsm, perc_attacked=perc_attacked, verbose=True)"
      ],
      "metadata": {
        "id": "9bX6Gx6H4NZR",
        "outputId": "41b98b41-4e77-475f-a2bf-50d351ced5bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model with attack:\n",
            "Epoch: 1 \tTraining Loss: 2.121610\n",
            "Epoch: 2 \tTraining Loss: 0.867504\n",
            "Epoch: 3 \tTraining Loss: 0.743201\n",
            "Epoch: 4 \tTraining Loss: 0.392083\n",
            "Epoch: 5 \tTraining Loss: 0.474755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. PGB L0"
      ],
      "metadata": {
        "id": "pSQ1O3WH7lku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Model with attack PGD L0:')\n",
        "# adverserial training with PGD\n",
        "model_withattackPGD = ConvModel()\n",
        "if cuda:\n",
        "  model_withattack = model_withattackPGD.cuda()\n",
        "# define your loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# define the optimizer\n",
        "opt = optim.Adam(model_withattackPGD.parameters(), lr=0.001)\n",
        "#define attack\n",
        "eps = 0.1\n",
        "pdg_l0 = ProjectedGradientDescent(model_withattackPGD, 0.1, 0.001, 2)\n",
        "#define % attacked of the train dataset\n",
        "perc_attacked = 1\n",
        "# define epoch\n",
        "epochs = 5\n",
        "\n",
        "train_losses = adversarial_train_model(model_withattackPGD, criterion, opt, train_loader, epochs, attack=pdg_l0, perc_attacked=perc_attacked, verbose=True)"
      ],
      "metadata": {
        "id": "vSABc4wa7zjf",
        "outputId": "2d87958d-8601-4786-e3b8-e2010fd2c36e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model with attack PGD L0:\n",
            "Epoch: 1 \tTraining Loss: 1.445464\n",
            "Epoch: 2 \tTraining Loss: 1.476078\n",
            "Epoch: 3 \tTraining Loss: 1.365985\n",
            "Epoch: 4 \tTraining Loss: 1.000553\n",
            "Epoch: 5 \tTraining Loss: 0.970175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. PGD L2"
      ],
      "metadata": {
        "id": "KRso6YIU8nVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Model with attack PGD L2:')\n",
        "# adverserial training with PGD\n",
        "model_withattackPGDL2 = ConvModel()\n",
        "if cuda:\n",
        "  model_withattack = model_withattackPGDL2.cuda()\n",
        "# define your loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# define the optimizer\n",
        "opt = optim.Adam(model_withattackPGDL2.parameters(), lr=0.001)\n",
        "#define attack\n",
        "eps = 0.1\n",
        "pdg_l2 = ProjectedGradientDescent(model_withattackPGDL2, 0.1, 0.001, 2)\n",
        "#define % attacked of the train dataset\n",
        "perc_attacked = 1\n",
        "# define epoch\n",
        "epochs = 5\n",
        "\n",
        "train_losses = adversarial_train_model(model_withattackPGDL2, criterion, opt, train_loader, epochs, attack=pdg_l2, perc_attacked=perc_attacked, verbose=True)"
      ],
      "metadata": {
        "id": "P1WaZbVp8okJ",
        "outputId": "ce7a9c63-6533-4528-b553-e3ea8f56173a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model with attack PGD L2:\n",
            "Epoch: 1 \tTraining Loss: 1.372156\n",
            "Epoch: 2 \tTraining Loss: 1.281798\n",
            "Epoch: 3 \tTraining Loss: 1.194507\n",
            "Epoch: 4 \tTraining Loss: 0.938201\n",
            "Epoch: 5 \tTraining Loss: 1.144774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. PGD Linf\n"
      ],
      "metadata": {
        "id": "Bipq8jR886Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Model with attack PGD Linf:')\n",
        "# adverserial training with PGD\n",
        "model_withattackPGDLinf = ConvModel()\n",
        "if cuda:\n",
        "  model_withattack = model_withattackPGDLinf.cuda()\n",
        "# define your loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# define the optimizer\n",
        "opt = optim.Adam(model_withattackPGDLinf.parameters(), lr=0.001)\n",
        "#define attack\n",
        "eps = 0.1\n",
        "pdg_linf = ProjectedGradientDescent(model_withattackPGDLinf, 0.1, 0.001, 2)\n",
        "#define % attacked of the train dataset\n",
        "perc_attacked = 1\n",
        "# define epoch\n",
        "epochs = 5\n",
        "\n",
        "train_losses = adversarial_train_model(model_withattackPGDLinf, criterion, opt, train_loader, epochs, attack=pdg_linf, perc_attacked=perc_attacked, verbose=True)"
      ],
      "metadata": {
        "id": "SJDF60_K875b",
        "outputId": "8b211900-5466-445b-bd34-6aa09f153ee7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model with attack PGD Linf:\n",
            "Epoch: 1 \tTraining Loss: 1.389466\n",
            "Epoch: 2 \tTraining Loss: 1.487089\n",
            "Epoch: 3 \tTraining Loss: 1.142921\n",
            "Epoch: 4 \tTraining Loss: 1.002026\n",
            "Epoch: 5 \tTraining Loss: 1.148413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B. Evaluating Robustness vs. Accuracy Tradeoff"
      ],
      "metadata": {
        "id": "mccgtYXK9VQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metrics to Consider**"
      ],
      "metadata": {
        "id": "V6n98afQ9YmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, loader, attack=None, attack_name=''):\n",
        "  \"\"\"Function to evaluate your model on a specific loader\"\"\"\n",
        "  accuracy = 0.\n",
        "  n_inputs = 0.\n",
        "  for n_batch, (imgs, labels) in enumerate(loader):\n",
        "    if cuda:\n",
        "      imgs, labels = imgs.cuda(), labels.cuda()\n",
        "    if attack is None:\n",
        "      outputs = model(imgs)\n",
        "    else:\n",
        "      delta = attack.compute(imgs, labels)\n",
        "      adv = imgs + delta\n",
        "      outputs = model(adv)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    accuracy += predicted.eq(labels.data).cpu().sum().numpy()\n",
        "    n_inputs += imgs.shape[0]\n",
        "  accuracy /= n_inputs\n",
        "\n",
        "  if attack is None:\n",
        "    print('Clean Accuracy, accuracy on testset: {:.4f}'.format(accuracy))\n",
        "  else:\n",
        "    print('Robust Accuracy, accuracy on testset with attack {}: {:.4f}'.format(attack_name, accuracy))\n",
        ""
      ],
      "metadata": {
        "id": "6M6Yfz2G9Xpq"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tradeoff Insights**\n",
        "\n",
        "1. On model without adversarial training"
      ],
      "metadata": {
        "id": "dChiHpND9qaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean Accuracy: eval model on not attacked test set\n",
        "eval_model(model_noattack, test_loader)\n",
        "#Robust Accuracy: eval model attacked test set\n",
        "eval_model(model_noattack, test_loader, fgsm, 'FGSM')\n",
        "eval_model(model_noattack, test_loader, pdg_l0, 'PGD L0')\n",
        "eval_model(model_noattack, test_loader, pdg_l2, 'PGD L2')\n",
        "eval_model(model_noattack, test_loader, pdg_linf, 'PGD Linf')"
      ],
      "metadata": {
        "id": "vqpQ6tEb9psr",
        "outputId": "cc36fefb-bae2-4a8c-a82c-5cc8b8ebec31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean Accuracy, accuracy on testset: 0.7300\n",
            "Robust Accuracy, accuracy on testset with attack FGSM: 0.0992\n",
            "Robust Accuracy, accuracy on testset with attack PGD L0: 0.6977\n",
            "Robust Accuracy, accuracy on testset with attack PGD L2: 0.6972\n",
            "Robust Accuracy, accuracy on testset with attack PGD Linf: 0.6974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. On model with adversarial dedicated adversarial training"
      ],
      "metadata": {
        "id": "LmoIVAMe_5_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean Accuracy: eval model on not attacked test set\n",
        "eval_model(model_withattackFGSM, test_loader)\n",
        "eval_model(model_withattackPGD, test_loader)\n",
        "eval_model(model_withattackPGDL2, test_loader)\n",
        "eval_model(model_withattackPGDLinf, test_loader)\n",
        "\n",
        "#Robust Accuracy: eval model attacked test set\n",
        "eval_model(model_withattackFGSM, test_loader, fgsm, 'FGSM')\n",
        "eval_model(model_withattackPGD, test_loader, pdg_l0, 'PGD L0')\n",
        "eval_model(model_withattackPGDL2, test_loader, pdg_l2, 'PGD L2')\n",
        "eval_model(model_withattackPGDLinf, test_loader, pdg_linf, 'PGD Linf')"
      ],
      "metadata": {
        "id": "RmASHeVE_YVN",
        "outputId": "87372709-ddab-41a3-d895-6167723f563e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean Accuracy, accuracy on testset: 0.6012\n",
            "Clean Accuracy, accuracy on testset: 0.7306\n",
            "Clean Accuracy, accuracy on testset: 0.7221\n",
            "Clean Accuracy, accuracy on testset: 0.7102\n",
            "Robust Accuracy, accuracy on testset with attack FGSM: 0.8686\n",
            "Robust Accuracy, accuracy on testset with attack PGD L0: 0.6638\n",
            "Robust Accuracy, accuracy on testset with attack PGD L2: 0.6500\n",
            "Robust Accuracy, accuracy on testset with attack PGD Linf: 0.6433\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}